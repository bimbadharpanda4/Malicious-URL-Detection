# -*- coding: utf-8 -*-
"""biLSTM BERT 14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z2kAaIUVaaajSPtqJ99u0EVUh0PUsBg4
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import hashlib
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from transformers import BertTokenizer, BertModel



# Load dataset
df = pd.read_csv("/content/phishing_dataset3.csv")
df.shape

df.sample(5)

# ------------------- Clean data -------------------
df['MissingTitle'] = df['MissingTitle'].fillna("Unknown")
df.drop_duplicates(inplace=True)
df['CLASS_LABEL'] = df['CLASS_LABEL'].astype(int)
print("Unique values in 'CLASS_LABEL':", df['CLASS_LABEL'].unique())

# ------------------- URL Encoding -------------------
def url_to_numerical(url):
    return [ord(char) for char in str(url)]

df['numerical_url'] = df['id'].apply(url_to_numerical)

# ------------------- 2D Bloom Filter -------------------
class BloomFilter2D:
    def __init__(self, rows, cols, hash_functions):
        self.rows = rows
        self.cols = cols
        self.hash_functions = hash_functions
        self.bit_matrix = [[0] * cols for _ in range(rows)]

    def add(self, item):
        for func in self.hash_functions:
            row_idx, col_idx = func(item)
            row_idx %= self.rows
            col_idx %= self.cols
            self.bit_matrix[row_idx][col_idx] = 1

    def contains(self, item):
        for func in self.hash_functions:
            row_idx, col_idx = func(item)
            row_idx %= self.rows
            col_idx %= self.cols
            if self.bit_matrix[row_idx][col_idx] == 0:
                return False
        return True

# Example hash functions for 2D indexing
def hash_func1(item):
    h = hashlib.md5(str(item).encode()).hexdigest()
    return int(h[:8], 16), int(h[8:16], 16)

def hash_func2(item):
    h = hashlib.sha1(str(item).encode()).hexdigest()
    return int(h[:8], 16), int(h[8:16], 16)

bloom_filter_2d = BloomFilter2D(rows=100, cols=100, hash_functions=[hash_func1, hash_func2])

# Add URLs to the Bloom filter
for numerical_url in df['numerical_url']:
    bloom_filter_2d.add(tuple(numerical_url))

# Create a feature based on presence in the Bloom filter
df['bloom_feature'] = df['numerical_url'].apply(lambda x: bloom_filter_2d.contains(tuple(x))).astype(int)

# ------------------- Train-test split -------------------
X = df.drop(columns=['CLASS_LABEL'])
y = df['CLASS_LABEL']
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# ------------------- BERT embeddings -------------------
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

def get_bert_embeddings(urls):
    encoded_input = tokenizer(urls, padding=True, truncation=True, return_tensors='pt')
    with torch.no_grad():
        model_output = model(**encoded_input)
    embeddings = model_output.last_hidden_state[:, 0, :].numpy()
    return embeddings

# Ensure URLs are strings
X_train['id'] = X_train['id'].astype(str)
X_val['id'] = X_val['id'].astype(str)
X_test['id'] = X_test['id'].astype(str)

# Generate embeddings
X_train_embeddings = get_bert_embeddings(X_train['id'].tolist())
X_val_embeddings = get_bert_embeddings(X_val['id'].tolist())
X_test_embeddings = get_bert_embeddings(X_test['id'].tolist())

# Combine embeddings + bloom feature
def combine_features(df_split, embeddings):
    other_features = df_split[['bloom_feature']].values
    return np.concatenate((other_features, embeddings), axis=1)

X_train_final = combine_features(X_train, X_train_embeddings)
X_val_final = combine_features(X_val, X_val_embeddings)
X_test_final = combine_features(X_test, X_test_embeddings)

class PhishingBiLSTM(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=1, num_classes=2, dropout=0.5):
        super(PhishingBiLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True,
                            bidirectional=True, dropout=dropout if num_layers > 1 else 0)
        self.fc1 = nn.Linear(hidden_size * 2, 128)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
        self.fc2 = nn.Linear(128, num_classes)

    def forward(self, x):
        x = x.unsqueeze(1)  # (Batch, Time=1, Feature)
        lstm_out, _ = self.lstm(x)
        lstm_out_last = lstm_out[:, -1, :]  # Last output
        x = self.relu(self.fc1(lstm_out_last))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# ------------------- DataLoader -------------------
X_train_tensor = torch.tensor(X_train_final, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)
X_val_tensor = torch.tensor(X_val_final, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val.values, dtype=torch.long)

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)

input_size = X_train_final.shape[1]
num_classes = 2
model_bilstm = PhishingBiLSTM(input_size, hidden_size=64, num_layers=1, num_classes=num_classes)
optimizer = optim.Adam(model_bilstm.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

num_epochs = 50
patience = 5
best_val_loss = float('inf')
epochs_no_improve = 0

train_accuracies = []
val_accuracies = []
train_losses = []
val_losses = []

for epoch in range(num_epochs):
    model_bilstm.train()
    correct = 0
    total = 0
    running_loss = 0.0
    for data, target in train_loader:
        optimizer.zero_grad()
        output = model_bilstm(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * data.size(0)
        pred = output.argmax(dim=1)
        correct += (pred == target).sum().item()
        total += target.size(0)
    train_acc = 100. * correct / total
    train_loss = running_loss / total
    train_accuracies.append(train_acc)
    train_losses.append(train_loss)

    # Validation
    model_bilstm.eval()
    correct = 0
    total = 0
    running_loss = 0.0
    with torch.no_grad():
        for data, target in val_loader:
            output = model_bilstm(data)
            loss = criterion(output, target)
            running_loss += loss.item() * data.size(0)
            pred = output.argmax(dim=1)
            correct += (pred == target).sum().item()
            total += target.size(0)
    val_acc = 100. * correct / total
    val_loss = running_loss / total
    val_accuracies.append(val_acc)
    val_losses.append(val_loss)

    print(f"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, "
          f"Train Acc = {train_acc:.2f}%, Val Acc = {val_acc:.2f}%")

    # Early stopping check
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        epochs_no_improve = 0
        best_model_state = model_bilstm.state_dict()
    else:
        epochs_no_improve += 1
        if epochs_no_improve >= patience:
            print("Early stopping triggered.")
            model_bilstm.load_state_dict(best_model_state)
            break

# ------------------- Evaluation -------------------
X_test_tensor = torch.tensor(X_test_final, dtype=torch.float32)
with torch.no_grad():
    outputs = model_bilstm(X_test_tensor)
    preds = outputs.argmax(dim=1).numpy()

# Accuracy
accuracy = accuracy_score(y_test, preds)
print(f'Accuracy: {accuracy:.4f}')

# Classification Report
print("\nClassification Report:\n", classification_report(y_test, preds))

# ------------------- Accuracy & Loss Plots -------------------
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(train_accuracies, label='Train Accuracy')
plt.plot(val_accuracies, label='Validation Accuracy')
plt.title('Accuracy vs Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.title('Loss vs Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

cm = confusion_matrix(y_test, preds)

plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
  xticklabels=np.unique(y_test),
  yticklabels=np.unique(y_test))

plt.xlabel('Predicted Label')
plt.ylabel('Actual Label')
plt.title('Confusion Matrix')
plt.show()